<div class="timeline-item">
    <div class="timeline-content">
        <button class="btn-close" onclick="toggleProject(this)">‚úï</button>

        <div class="project-image">
            <img src="assets/images/project-AI/eda_cover.png" alt="RL Optimization for Standard Cells">
        </div>

        <div class="project-info">
            <h3>
                <span class="lang-text" data-lang="fr">Optimisation RL sur Standard Cells</span>
                <span class="lang-text" data-lang="en" style="display: none;">RL Optimization for Standard Cells</span>
            </h3>

            <p class="lang-text" data-lang="fr">
                D√©veloppement d'un outil d'optimisation de dimensionnement de transistors (<strong>Sizing</strong>) par <strong>Intelligence Artificielle</strong>. 
                Une approche exploratoire allant de la mod√©lisation physique (<strong>SPICE</strong>) √† l'agent autonome (<strong>PPO</strong>), avec une interface d√©di√©e √† l'exp√©rimentation.
            </p>
            <p class="lang-text" data-lang="en" style="display: none;">
                Development of an <strong>AI-based</strong> transistor sizing optimization tool. 
                An exploratory approach ranging from physical modeling (<strong>SPICE</strong>) to autonomous agents (<strong>PPO</strong>), featuring a dedicated experimentation interface.
            </p>

            <div class="project-tags">
                <span class="tag">Python</span>
                <span class="tag">PyQt6</span>
                <span class="tag">NGSpice</span>
                <span class="tag">Stable-Baselines3</span>
                <span class="tag">Sky130 PDK</span>
                <span class="tag">Reinforcement Learning</span>
            </div>

            <div class="card-buttons">
                <button class="btn-expand" onclick="toggleProject(this)">
                    <span class="lang-text" data-lang="fr">En savoir plus ‚Üí</span>
                    <span class="lang-text" data-lang="en" style="display: none;">Learn more ‚Üí</span>
                </button>

                <a href="https://github.com/Vincent-Cauquil/5A-Projet-Optimisation-de-Standard-Cells" class="btn-github" target="_blank">
                    <svg class="icon"><use href="#icon-github"/></svg> GitHub
                </a>
            </div>

            <div class="project-details">

                <!-- ===== BLOC 1 : Environnement Python-SPICE ===== -->
                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr"> Construction de l'Environnement (Le Pont Python-SPICE)</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;"> Building the Environment (The Python-SPICE Bridge)</h4>

                    <div class="evidence-container">
                        <div class="evidence-text">

                            <div class="lang-text" data-lang="fr">
                                <p><strong>La D√©marche :</strong> Avant de faire de l'IA, il fallait "parler" au simulateur. J'ai cr√©√© un environnement <strong>custom compatible Gymnasium</strong> capable d'interagir avec <strong>NGSpice</strong>.</p>
                                
                                <p><strong>D√©fis Techniques :</strong></p>
                                <ul>
                                    <li><strong>G√©n√©ration dynamique de Netlists SPICE</strong> : Injection des param√®tres <code>W/L</code> (largeur/longueur des transistors) pour chaque essai de l'agent.</li>
                                    <li><strong>Parsing robuste des fichiers <code>.raw</code></strong> : Extraction automatique des m√©triques (d√©lais de propagation, consommation √©nerg√©tique).</li>
                                    <li><strong>Optimisation Critique</strong> : Le RL n√©cessitant <strong>des milliers de simulations</strong>, j'ai parall√©lis√© le processus avec <strong>Python Multiprocessing</strong>, r√©duisant le temps d'entra√Ænement de <strong>plusieurs heures</strong> √† <strong>quelques minutes</strong>.</li>
                                </ul>
                            </div>

                            <div class="lang-text" data-lang="en" style="display: none;">
                                <p><strong>The Approach:</strong> Before tackling AI, I needed to "talk" to the simulator. I built a <strong>custom Gymnasium-compatible environment</strong> to interact with <strong>NGSpice</strong>.</p>
                                
                                <p><strong>Technical Challenges:</strong></p>
                                <ul>
                                    <li><strong>Dynamic SPICE Netlist Generation</strong>: Injecting <code>W/L</code> parameters (transistor width/length) for each agent trial.</li>
                                    <li><strong>Robust <code>.raw</code> File Parsing</strong>: Automated extraction of metrics (propagation delays, power consumption).</li>
                                    <li><strong>Critical Optimization</strong>: Since RL requires <strong>thousands of simulations</strong>, I parallelized the process using <strong>Python Multiprocessing</strong>, cutting training time from <strong>hours to minutes</strong>.</li>
                                </ul>
                            </div>

                        </div>

                        <div class="evidence-image">
                            <img class="img-translatable" 
                                 data-src-fr="assets/images/project-AI/eda_architecture_fr.svg"
                                 data-src-en="assets/images/project-AI/eda_architecture_en.svg"
                                 data-alt-fr="Processus de Microfabrication"
                                 data-alt-en="Microfabrication Process"
                                 src="assets/images/project-AI/eda_architecture_fr.svg" 
                                 alt="Processus de Microfabrication"
                                 style="background:white; padding:10px; border-radius:10px;">
                            
                            <span class="caption lang-text" data-lang="fr">Architecture : UI, Middleware et Backend IA</span>
                            <span class="caption lang-text" data-lang="en" style="display: none;">Architecture: UI, Middleware, and AI Backend</span>
                        </div>   
                    </div>
                </div>

                <!-- ===== BLOC 2 : Apprentissage RL & UX ===== -->
                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr"> Apprentissage & Interface d'Exp√©rimentation</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;"> Learning & Experimentation Interface</h4>

                    <div class="evidence-container">
                        <div class="evidence-text">
                            <div class="lang-text" data-lang="fr">
                                <p><strong>L'Agent IA :</strong> Utilisation de <strong>PPO</strong> (Proximal Policy Optimization) via <strong>Stable-Baselines3</strong>. Le travail crucial a consist√© √† d√©finir une <strong>fonction de Reward √©quilibr√©e</strong> (Trade-off Power vs Delay) pour que l'agent converge vers une solution viable.</p>
                                
                                <p><strong>UX Param√©trable :</strong> Pour comprendre le comportement "bo√Æte noire" de l'IA, j'ai d√©velopp√© une <strong>GUI en PyQt6</strong>. Elle permet de :</p>
                                <ul>
                                    <li>Modifier les <strong>hyperparam√®tres</strong> (Learning rate, Timesteps, Œ≥) en temps r√©el.</li>
                                    <li>Ajuster les <strong>contraintes physiques du PDK Sky130</strong> (Vdd, Tox, Limites W/L).</li>
                                    <li>Visualiser les <strong>courbes de convergence</strong> pendant l'entra√Ænement.</li>
                                </ul>
                                <p>Cette interface transforme le code en <strong>v√©ritable laboratoire d'exp√©rimentation</strong>, essentiel pour it√©rer rapidement.</p>
                            </div>

                            <div class="lang-text" data-lang="en" style="display: none;">
                                <p><strong>AI Agent:</strong> Using <strong>PPO</strong> (Proximal Policy Optimization) via <strong>Stable-Baselines3</strong>. The key work was defining a <strong>balanced Reward function</strong> (Power vs Delay trade-off) to ensure convergence to viable solutions.</p>
                                
                                <p><strong>Configurable UX:</strong> To understand the "black box" behavior, I built a <strong>PyQt6 GUI</strong>. It allows:</p>
                                <ul>
                                    <li>Real-time modification of <strong>hyperparameters</strong> (Learning rate, Timesteps, Œ≥).</li>
                                    <li>Adjustment of <strong>Sky130 PDK physical constraints</strong> (Vdd, Tox, W/L limits).</li>
                                    <li>Visualization of <strong>convergence curves</strong> during training.</li>
                                </ul>
                                <p>This interface turns the code into a <strong>true experimentation lab</strong>, essential for rapid iteration.</p>
                            </div>
                        </div>

                        <div class="evidence-slider" id="eda-slider">
                            <!-- Slide 1 : Interface PyQt6 -->
                            <div class="slide active">
                                <img src="assets/images/project-AI/eda_gui.png" 
                                     alt="PyQt6 Configurable Interface">
                                <span class="caption lang-text" data-lang="fr">1/3 : Interface UX param√©trable (PyQt6) pour piloter l'entra√Ænement</span>
                                <span class="caption lang-text" data-lang="en" style="display: none;">1/3 : Configurable UX Interface (PyQt6) to control training</span>
                            </div>
                        
                            <!-- Slide 2 : Courbes d'apprentissage -->
                            <div class="slide">
                                <img src="assets/images/project-AI/eda_training.png" 
                                     alt="RL Training Curves">
                                <span class="caption lang-text" data-lang="fr">2/3 : Convergence de l'agent (Reward Maximization)</span>
                                <span class="caption lang-text" data-lang="en" style="display: none;">2/3 : Agent Convergence (Reward Maximization)</span>
                            </div>
                        
                            <!-- Slide 3 : Sch√©matique optimis√© -->
                            <div class="slide">
                                <img src="assets/images/project-AI/eda_schematic.png" 
                                     alt="Optimized Inverter Schematic">
                                <span class="caption lang-text" data-lang="fr">3/3 : R√©sultat : Cellule Standard optimis√©e (Schematic)</span>
                                <span class="caption lang-text" data-lang="en" style="display: none;">3/3 : Result: Optimized Standard Cell (Schematic)</span>
                            </div>
                        
                            <a class="prev" onclick="plusSlides(-1, 'eda-slider')">‚ùÆ</a>
                            <a class="next" onclick="plusSlides(1, 'eda-slider')">‚ùØ</a>
                        </div>                        
                    </div>
                </div>

                <!-- ===== BLOC 3 : R√©sultats & Perspectives ===== -->
                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr">üìä R√©sultats & Perspectives</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;">üìä Results & Perspectives</h4>

                    <div class="evidence-container">
                        <div class="evidence-text">
                            <div class="lang-text" data-lang="fr">
                                <p><strong>Performance Obtenue :</strong> L'agent a r√©ussi √† optimiser un <strong>inverseur CMOS</strong> en respectant les contraintes du <strong>PDK Sky130</strong>. La cellule g√©n√©r√©e pr√©sente un compromis <strong>D√©lai/Puissance</strong> comp√©titif par rapport aux m√©thodes manuelles classiques.</p>
                                
                                <p><strong>Limites Identifi√©es :</strong></p>
                                <ul>
                                    <li><strong>Temps de simulation SPICE</strong> : Malgr√© la parall√©lisation, le goulot d'√©tranglement reste la dur√©e des simulations transistor-level.</li>
                                    <li><strong>Scalabilit√©</strong> : Le passage √† des cellules plus complexes (NOR, NAND multi-inputs) n√©cessiterait un espace d'√©tat bien plus large.</li>
                                </ul>
                                
                                <p><strong>Pistes d'Am√©lioration :</strong></p>
                                <ul>
                                    <li>Utiliser des <strong>mod√®les de substitution (Surrogate Models)</strong> pour remplacer SPICE par des approximations rapides (R√©gression Polynomiale, R√©seaux de Neurones).</li>
                                    <li>Explorer des algorithmes RL plus modernes comme <strong>SAC</strong> (Soft Actor-Critic) pour am√©liorer l'exploration.</li>
                                </ul>
                            </div>

                            <div class="lang-text" data-lang="en" style="display: none;">
                                <p><strong>Performance Achieved:</strong> The agent successfully optimized a <strong>CMOS inverter</strong> while respecting <strong>Sky130 PDK</strong> constraints. The generated cell shows a competitive <strong>Delay/Power</strong> trade-off compared to traditional manual methods.</p>
                                
                                <p><strong>Identified Limitations:</strong></p>
                                <ul>
                                    <li><strong>SPICE Simulation Time</strong>: Despite parallelization, transistor-level simulation remains the bottleneck.</li>
                                    <li><strong>Scalability</strong>: Moving to more complex cells (multi-input NOR, NAND) would require a significantly larger state space.</li>
                                </ul>
                                
                                <p><strong>Improvement Paths:</strong></p>
                                <ul>
                                    <li>Use <strong>Surrogate Models</strong> to replace SPICE with fast approximations (Polynomial Regression, Neural Networks).</li>
                                    <li>Explore modern RL algorithms like <strong>SAC</strong> (Soft Actor-Critic) for better exploration.</li>
                                </ul>
                            </div>
                        </div>

                        <div class="evidence-image">
                            <img src="assets/images/project-AI/eda_results.png" 
                                 alt="Comparative Results">
                            <span class="caption lang-text" data-lang="fr">Comparaison : Sizing Manuel vs Optimis√© par RL</span>
                            <span class="caption lang-text" data-lang="en" style="display: none;">Comparison : Manual Sizing vs RL-Optimized</span>
                        </div>
                    </div>
                </div>

                <!-- ===== BLOC 4 : Comp√©tences Mobilis√©es ===== -->
                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr">Comp√©tences Mobilis√©es</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;">Skills Leveraged</h4>

                    <div class="skills-grid">
                        <div class="skill-category">
                            <h5 class="lang-text" data-lang="fr">Machine Learning</h5>
                            <h5 class="lang-text" data-lang="en" style="display: none;">Machine Learning</h5>
                            <ul>
                                <li>Reinforcement Learning (PPO)</li>
                                <li>Stable-Baselines3</li>
                                <li>Reward Engineering</li>
                                <li>Hyperparameter Tuning</li>
                            </ul>
                        </div>

                        <div class="skill-category">
                            <h5 class="lang-text" data-lang="fr">Design EDA</h5>
                            <h5 class="lang-text" data-lang="en" style="display: none;">EDA Design</h5>
                            <ul>
                                <li>NGSpice (Simulation SPICE)</li>
                                <li>Sky130 PDK (Skywater)</li>
                                <li>Netlist Generation</li>
                                <li>Transistor Sizing</li>
                            </ul>
                        </div>

                        <div class="skill-category">
                            <h5 class="lang-text" data-lang="fr">Software Engineering</h5>
                            <h5 class="lang-text" data-lang="en" style="display: none;">Software Engineering</h5>
                            <ul>
                                <li>Python (OOP, Multiprocessing)</li>
                                <li>PyQt6 (Interface Graphique)</li>
                                <li>Gymnasium (Custom Env)</li>
                                <li>Data Parsing (.raw files)</li>
                            </ul>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <div class="timeline-date">
        <span class="date">
            <span class="lang-text" data-lang="fr">D√©c - Sept 2025</span>
            <span class="lang-text" data-lang="en" style="display: none;">Dec - Sept 2025</span>
        </span>
        <div class="timeline-dot"></div>
    </div>
</div>
