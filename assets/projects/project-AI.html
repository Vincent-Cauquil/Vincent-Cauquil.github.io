<div class="timeline-item">
    <div class="timeline-content">
        <button class="btn-close" onclick="toggleProject(this)">✕</button>

        <div class="project-image">
            <img src="assets/images/projects/eda_cover.png" alt="RL Optimization for Standard Cells">
        </div>

        <div class="project-info">
            <h3>Optimisation RL sur Standard Cells</h3>

            <p class="lang-text" data-lang="fr">
                Développement d'un outil d'optimisation de dimensionnement de transistors (Sizing) par IA.
                Une approche exploratoire allant de la modélisation physique (SPICE) à l'agent autonome (PPO), avec une interface dédiée à l'expérimentation.
            </p>
            <p class="lang-text" data-lang="en" style="display: none;">
                Development of an AI-based transistor sizing optimization tool.
                An exploratory approach ranging from physical modeling (SPICE) to autonomous agents (PPO), featuring a dedicated interface for experimentation.
            </p>

            <div class="project-tags">
                <span class="tag">Python</span>
                <span class="tag">PyQt6</span>
                <span class="tag">NGSpice</span>
                <span class="tag">Stable-Baselines3</span>
                <span class="tag">Sky130 PDK</span>
            </div>

            <div class="card-buttons">
                <button class="btn-expand" onclick="toggleProject(this)">
                    <span class="lang-text" data-lang="fr">Voir la démarche & architecture →</span>
                    <span class="lang-text" data-lang="en" style="display: none;">See approach & architecture →</span>
                </button>
                
                <a href="https://github.com/EnderCryme/5A-Projet-Optimisation-de-Standard-Cells" class="btn-github" target="_blank">
                    <svg class="icon"><use href="#icon-github"/></svg> GitHub
                </a>
            </div>

            <div class="project-details">

                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr">1. Construction de l'Environnement (Le Pont Python-SPICE)</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;">1. Building the Environment (The Python-SPICE Bridge)</h4>
                    
                    <div class="evidence-container">
                        <div class="evidence-text">
                            <p class="lang-text" data-lang="fr">
                                <strong>La démarche :</strong> Avant de faire de l'IA, il fallait "parler" au simulateur. J'ai dû créer un environnement custom compatible <em>Gymnasium</em> capable d'interagir avec <strong>NGSpice</strong>.<br><br>
                                <strong>Défis techniques :</strong>
                                <ul style="margin-top:5px; margin-bottom:10px; padding-left:15px; font-size:0.9em; color:var(--text-muted);">
                                    <li>Génération dynamique de Netlists SPICE (injection des paramètres W/L).</li>
                                    <li>Parsing robuste des fichiers de sortie <code>.raw</code> pour extraire délais et consommation.</li>
                                    <li><strong>Optimisation :</strong> Le RL nécessitant des milliers d'essais, j'ai dû paralléliser les simulations (Multiprocessing) pour réduire le temps d'entraînement de plusieurs heures à quelques minutes.</li>
                                </ul>
                            </p>
                            <p class="lang-text" data-lang="en" style="display: none;">
                                <strong>The Approach:</strong> Before AI, we needed to "talk" to the simulator. I created a custom <em>Gymnasium</em>-compatible environment to interact with <strong>NGSpice</strong>.<br><br>
                                <strong>Technical Challenges:</strong>
                                <ul style="margin-top:5px; margin-bottom:10px; padding-left:15px; font-size:0.9em; color:var(--text-muted);">
                                    <li>Dynamic SPICE Netlist generation (W/L parameter injection).</li>
                                    <li>Robust parsing of <code>.raw</code> output files to extract delays and power.</li>
                                    <li><strong>Optimization:</strong> Since RL needs thousands of steps, I parallelized simulations (Multiprocessing) to cut training time from hours to minutes.</li>
                                </ul>
                            </p>
                        </div>
                        <div class="evidence-image">
                            <img src="assets/images/projects/eda_architecture.svg" alt="Architecture Logicielle Pipeline EDA" style="background:white; padding:10px; border-radius:10px;">
                            <span class="caption">Pipeline optimisé : De la Netlist au Reward</span>
                        </div>
                    </div>
                </div>

                <div class="detail-block">
                    <h4 class="lang-text" data-lang="fr">2. Apprentissage & Interface d'Expérimentation (UX)</h4>
                    <h4 class="lang-text" data-lang="en" style="display: none;">2. Learning & Experimentation Interface (UX)</h4>

                    <div class="evidence-container">
                        <div class="evidence-text">
                            <p class="lang-text" data-lang="fr">
                                <strong>L'Agent IA :</strong> Utilisation de PPO (Proximal Policy Optimization). Le travail a consisté à définir une fonction de <em>Reward</em> équilibrée (Trade-off Power vs Delay) pour que l'agent converge vers une solution viable.<br><br>
                                <strong>UX Paramétrable :</strong> Pour comprendre le comportement "boîte noire" de l'IA, j'ai développé une GUI en <strong>PyQt6</strong>. Elle permet de modifier les hyperparamètres (Learning rate, Timesteps) et les contraintes physiques (PDK Sky130) en temps réel, transformant le code en véritable laboratoire d'expérimentation.
                            </p>
                            <p class="lang-text" data-lang="en" style="display: none;">
                                <strong>AI Agent:</strong> Using PPO (Proximal Policy Optimization). The key work was defining a balanced <em>Reward</em> function (Power vs Delay trade-off) to ensure convergence.<br><br>
                                <strong>Configurable UX:</strong> To understand the "black box" behavior, I built a <strong>PyQt6</strong> GUI. It allows real-time modification of hyperparameters (Learning rate, Timesteps) and physical constraints (Sky130 PDK), turning the code into a true experimentation lab.
                            </p>
                        </div>

                        <div class="evidence-slider" id="eda-slider">
                            <div class="slide active">
                                <img src="assets/images/projects/eda_gui.png" alt="Interface PyQt6 Configurable">
                                <span class="caption">1/3 : Interface UX paramétrable (PyQt6) pour piloter l'entraînement</span>
                            </div>
                            <div class="slide">
                                <img src="assets/images/projects/eda_training.png" alt="Courbes d'apprentissage RL">
                                <span class="caption">2/3 : Convergence de l'agent (Reward Maximization)</span>
                            </div>
                            <div class="slide">
                                <img src="assets/images/projects/eda_schematic.png" alt="Schematic Inverter Optimized">
                                <span class="caption">3/3 : Résultat : Cellule Standard optimisée (Schematic)</span>
                            </div>
                            
                            <a class="prev" onclick="plusSlides(-1, 'eda-slider')">❮</a>
                            <a class="next" onclick="plusSlides(1, 'eda-slider')">❯</a>
                        </div>
                    </div>
                </div>

            </div> </div>
    </div>

    <div class="timeline-date">
        <span class="date">2024 - 2025</span>
        <div class="timeline-dot"></div>
    </div>
</div>
